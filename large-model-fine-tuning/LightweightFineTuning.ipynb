{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "In this experiment a foundation model reads a given review text and makes a meaning of the given input, making it suitable for estimating a numeric score related to that review text\n",
    "\n",
    "* __Fine-tuning dataset__: [yelp_review_full](https://huggingface.co/datasets/Yelp/yelp_review_full) containing review texts and corresponding 5 star ratings.\n",
    "* __Model__: [DistilBERT base model - uncased](https://huggingface.co/distilbert/distilbert-base-uncased) a model that processes a prompt and estimates appropriate next words\n",
    "* __Evaluation approach__: Since the model should estimate the star rating that can range from 1..5, common classification metrics are used: mainly cross entropy and optional accuracy, precision.\n",
    "* __PEFT technique__: Fine tuning the full model, only the trailing layer and LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "First load the foundation model, tokenizer and the dataset from huggingface.co. Since this is a relatively huge dataset, only a fraction of its content is used to demonstrate the techniques. The share is adjustable in parameter `dataset_size` that equally affects training and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading model + dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] size of: training set=65000 test set=5000\n",
      "[info] Labels to estimate:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '1 star', 1: '2 star', 2: '3 stars', 3: '4 stars', 4: '5 stars'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"[info] Loading model + dataset\")\n",
    "DATASET_SIZE = \"10%\"  # Note: please adapt, this is a comparably huge dataset\n",
    "RAW_DATASET = {\"train\": None, \"test\": None}\n",
    "RAW_DATASET[\"train\"] = load_dataset(\"Yelp/yelp_review_full\", split=f\"train[:{DATASET_SIZE}]\")\n",
    "RAW_DATASET[\"test\"] = load_dataset(\"Yelp/yelp_review_full\", split=f\"test[:{DATASET_SIZE}]\")\n",
    "\n",
    "LABELS = RAW_DATASET[\"train\"].features[\"label\"].names\n",
    "\n",
    "MODEL = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=len(LABELS))\n",
    "\n",
    "print(f\"[info] size of: training set={len(RAW_DATASET['train'])} test set={len(RAW_DATASET['test'])}\")\n",
    "print(\"[info] Labels to estimate:\")\n",
    "CLASSNUM_TO_LABEL = {i: l for i, l in enumerate(LABELS)}\n",
    "CLASSNUM_TO_LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e46ea",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Next turn the still human readable review text into a dataset of a format that becomes processible by the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Tokenize dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcab09424cf24ec2b16215eb1e47dc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3d11a323084692a6fc8051beebc632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 65000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[info] Tokenize dataset\")\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "TOKENIZED_DS = dict()\n",
    "for split in RAW_DATASET.keys():\n",
    "    TOKENIZED_DS[split] = RAW_DATASET[split].map(\n",
    "        lambda x: TOKENIZER(x[\"text\"], truncation=True, padding=\"max_length\")\n",
    "    )\n",
    "\n",
    "TOKENIZED_DS[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a1e99",
   "metadata": {},
   "source": [
    "### A peek into the data\n",
    "Now that all is prepared, let's have a look what we're working with. The predicition stems from the vanilla DistilBERT model, adjusted to only output a star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been to this restaurant twice and was disappointed both times. I won't go back. The first time we were there almost 3 hours. It took forever to order and then forever for our food to come and the place was empty. When I complained the manager was very rude and tried to blame us for taking to long to order. It made no sense, how could we order when the waitress wasn't coming to the table? After arguing with me he ended up taking $6 off of our $200+ bill. Ridiculous. If it were up to me I would have never returned. Unfortunately my family decided to go here again tonight. Again it took a long time to get our food. My food was cold and bland, my kids food was cold. My husbands salmon was burnt to a crisp and my sister in law took one bite of her trout and refused to eat any more because she claims it was so disgusting. The wedding soup and bread were good, but that's it! My drink sat empty throughout my meal and never got refilled even when I asked. Bad food, slow service and rude managers. I'll pass on this place if my family decides to go again. Not worth it at all with all the other good Italian options around.\n",
      "\n",
      "BERT prediction: 2 star GT: 1 star\n"
     ]
    }
   ],
   "source": [
    "TEST_LINE = TOKENIZER(TOKENIZED_DS[\"test\"][\"text\"][3], truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    PRED = MODEL(**TEST_LINE).logits\n",
    "    \n",
    "CASE = 3\n",
    "print(f'{RAW_DATASET[\"test\"][CASE][\"text\"]}\\n\\nBERT prediction: {CLASSNUM_TO_LABEL[torch.argmax(PRED).item()]} GT: {CLASSNUM_TO_LABEL[RAW_DATASET[\"test\"][CASE][\"label\"]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36f7f7",
   "metadata": {},
   "source": [
    "### Common function\n",
    "In the following functions are defined that are being used a couple of times for retraining & evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7e551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    centropy = torch.nn.functional.cross_entropy(torch.tensor(logits), torch.tensor(labels))\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions), \n",
    "            \"precision\": precision_score(labels, predictions, average='micro'), \n",
    "            \"eval_cross_entropy\": centropy.item()}\n",
    "\n",
    "def fine_tuning_pipeline(\n",
    "        model, \n",
    "        store_dir, \n",
    "        tokenized_ds = TOKENIZED_DS,\n",
    "        tokenizer = TOKENIZER,\n",
    "        no_training=False):\n",
    "    \"\"\"\n",
    "    This function provides essential Zynthian user guide information, \n",
    "    extracted from all <p></p> sections of its Wiki.\n",
    "    \n",
    "    Args:\n",
    "        model: the DNN that should be modified\n",
    "        store_dir: output directory information in order to separate the experiment outcomes\n",
    "        tokenized_ds: the machine processible database\n",
    "        tokenizer: the function to turn raw strings into tokens\n",
    "        no_training: freeze the model\n",
    "    Returns:\n",
    "        pd.DataFrame: cleaned DataFrame with the extracted data in the \"text\" column name\n",
    "    \"\"\"\n",
    "    \n",
    "    # define DNN pipeline\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=f\"/tmp/genai/lighweightfinetuning/{store_dir}\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=8, # adjust according to the GPU performance\n",
    "        per_device_eval_batch_size=8, # adjust according to the GPU performance\n",
    "        torch_empty_cache_steps=10,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"cross_entropy\",\n",
    "        label_names=[\"labels\"],\n",
    "        use_cpu=False # flip switch if your GPU is ready for it\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # execute inference\n",
    "    print(\"[info] running evaluation\")\n",
    "    with torch.no_grad():\n",
    "        pre_results = trainer.evaluate()  \n",
    "    if no_training:\n",
    "        return {\n",
    "            \"accuracy\": pre_results[\"eval_accuracy\"], \n",
    "            \"precision\": pre_results[\"eval_precision\"]\n",
    "        }\n",
    "    else:  \n",
    "        print(\"[info] retraining\")\n",
    "        trainer.train()\n",
    "        print(\"[info] evaluating retraining\")\n",
    "        post_results = trainer.evaluate()\n",
    "        model.save_pretrained(f\"{trainer.args.output_dir}/exported_model\", from_pt=True)\n",
    "\n",
    "        return {\n",
    "            \"pre training accuracy\": pre_results[\"eval_accuracy\"], \n",
    "            \"post training accuracy\": post_results[\"eval_precision\"],\n",
    "            \"pre training precision\": pre_results[\"eval_accuracy\"],\n",
    "            \"post training precision\": post_results[\"eval_precision\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "As mentioned in the beginning, sequentially a full retraining, a training limited to only one layer and a LoRA optimization will be conducted. This is to refelect the impact of the three strategies.\n",
    "\n",
    "### Retrain the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4659/1292456995.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] running evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 2:53:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8125' max='8125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8125/8125 2:49:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.602400</td>\n",
       "      <td>1.605779</td>\n",
       "      <td>1.605780</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.228200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] evaluating retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 03:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre training accuracy': 0.2022, 'post training accuracy': 0.2282, 'pre training precision': 0.2022, 'post training precision': 0.2282}\n"
     ]
    }
   ],
   "source": [
    "MODEL_FULL_TRAINING = deepcopy(MODEL)\n",
    "for param in MODEL_FULL_TRAINING.parameters():\n",
    "    param.requires_grad = True\n",
    "FULL_TRAINING_RESULT = fine_tuning_pipeline(MODEL_FULL_TRAINING, \"full\")\n",
    "print(FULL_TRAINING_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d454f",
   "metadata": {},
   "source": [
    "### Trailing layer training\n",
    "\n",
    "Here we're freezing the whole model except of one layer, so let's have a look how the model looks like and which layer should be picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34af7090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ONE_LAYER_TRAINING = deepcopy(MODEL)\n",
    "MODEL_ONE_LAYER_TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e99f04",
   "metadata": {},
   "source": [
    "Seems we're unfreezing layer `classifier` then, the one that has been resized to the number of star-rating to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] running evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4659/1292456995.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 55:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8125' max='8125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8125/8125 52:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.194000</td>\n",
       "      <td>1.150156</td>\n",
       "      <td>1.150156</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] evaluating retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 03:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre training accuracy': 0.2022, 'post training accuracy': 0.5018, 'pre training precision': 0.2022, 'post training precision': 0.5018}\n"
     ]
    }
   ],
   "source": [
    "for param in MODEL_ONE_LAYER_TRAINING.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in MODEL_ONE_LAYER_TRAINING.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "ONE_LAYER_TRAINING_RESULT = fine_tuning_pipeline(MODEL_ONE_LAYER_TRAINING, \"streamlined\")\n",
    "print(ONE_LAYER_TRAINING_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0811cb",
   "metadata": {},
   "source": [
    "### LoRA Training\n",
    "\n",
    "Compared to fine-tune the trailing classification layer, LoRA works well with attention layers hence `v` and `q` is picked for optimization below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] running evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4659/1292456995.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 2:20:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8125' max='8125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8125/8125 2:16:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.852800</td>\n",
       "      <td>0.871493</td>\n",
       "      <td>0.871493</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.622200</td>\n",
       "      <td>0.622200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] evaluating retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 03:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre training accuracy': 0.2022, 'post training accuracy': 0.6222, 'pre training precision': 0.2022, 'post training precision': 0.6222}\n"
     ]
    }
   ],
   "source": [
    "MODEL_LORA_TRAINING = deepcopy(MODEL)\n",
    "PEFT_CFG = LoraConfig(    \n",
    "    target_modules=['q_lin', 'v_lin'],\n",
    "    task_type='SEQ_CLS',\n",
    "    modules_to_save=[]\n",
    ")\n",
    "PEFT_MODEL = get_peft_model(model=MODEL_LORA_TRAINING, peft_config=PEFT_CFG)\n",
    "LORA_TRAINING_RESULT = fine_tuning_pipeline(PEFT_MODEL, \"lora\")\n",
    "print(LORA_TRAINING_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "Since every model has been saved persistently, it should be possible to load those exports and check if the evaluation results still match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4659/1292456995.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] running evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 03:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6222, 'precision': 0.6222}\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "LOADED_MODEL = AutoPeftModelForSequenceClassification.from_pretrained(\"/tmp/genai/lighweightfinetuning/lora/exported_model/\", config=PEFT_CFG, num_labels=len(LABELS))\n",
    "RESULTS = fine_tuning_pipeline(LOADED_MODEL, store_dir=\"/tmp\", no_training=True)\n",
    "print(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74adfde1",
   "metadata": {},
   "source": [
    "Let's check again with the statistics from directly after the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796e2464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre training accuracy': 0.2022, 'post training accuracy': 0.6222, 'pre training precision': 0.2022, 'post training precision': 0.6222}\n"
     ]
    }
   ],
   "source": [
    "print(LORA_TRAINING_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476064dc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "When retraining the full model the performance droped, it is likely to be related with `catastropihc forgetting`. In the opposite just retraining one single layer improved the data comprehension compared to its vanilla state. The results are best though with the LoRA method, where the model's attention layer was targeted.\n",
    "\n",
    "Eventually storing, loading and reevaluating once again showed the same results, showing a successfull persistent ex- and import of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
